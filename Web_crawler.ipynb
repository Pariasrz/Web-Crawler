{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "01502e93",
      "metadata": {
        "id": "01502e93"
      },
      "source": [
        "#Step 1: Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7aead812",
      "metadata": {
        "id": "7aead812"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "# Optional: Enable inline plotting for Jupyter\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41070d82",
      "metadata": {
        "id": "41070d82"
      },
      "source": [
        "#Step 2: Initialize the Web Crawler Parameters\n",
        "\n",
        "Here we define the starting URL and domain. We'll also initialize the graph and the BFS queue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "02f933c5",
      "metadata": {
        "id": "02f933c5"
      },
      "outputs": [],
      "source": [
        "# Starting page URL and domain\n",
        "start_url = 'https://example.com'\n",
        "domain = 'example.com'\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Define a queue and store the starting URL (BFS)\n",
        "queue = deque([(start_url, 0)])\n",
        "\n",
        "# Store visited URLs in a set to avoid revisiting\n",
        "visited = set()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "039d2d02",
      "metadata": {
        "id": "039d2d02"
      },
      "source": [
        "#Step 3: Web Crawling with BFS\n",
        "\n",
        "This step will perform the BFS crawling, adding nodes and edges to the graph for the URLs and links found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bf026236",
      "metadata": {
        "id": "bf026236"
      },
      "outputs": [],
      "source": [
        "# BFS for crawling the web\n",
        "while queue:\n",
        "    url, depth = queue.popleft()\n",
        "\n",
        "    # Skip URLs that are not on the specified domain or that have already been visited\n",
        "    if urlparse(url).netloc != domain or url in visited:\n",
        "        continue\n",
        "\n",
        "    # Mark the current URL as visited\n",
        "    visited.add(url)\n",
        "\n",
        "    # Fetch the HTML content of the current URL\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        html = response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Parse the HTML content using Beautiful Soup\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Add a node to the graph for the current URL\n",
        "    title = soup.title.string if soup.title else ''\n",
        "    G.add_node(url, label=title)\n",
        "\n",
        "    # Find all links on the page\n",
        "    links = soup.find_all('a')\n",
        "\n",
        "    # Add edges to the graph for each link on the page\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "\n",
        "        # Skip links with no href attribute\n",
        "        if not href:\n",
        "            continue\n",
        "\n",
        "        # Convert relative URLs to absolute URLs\n",
        "        href = urljoin(url, href)\n",
        "\n",
        "        # Add an edge to the graph for the link if it is on the specified domain\n",
        "        if urlparse(href).netloc == domain:\n",
        "            G.add_edge(url, href)\n",
        "\n",
        "            # Add the linked URL to the queue for crawling if its depth is within the limit\n",
        "            if depth < 2:\n",
        "                queue.append((href, depth + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fb60124",
      "metadata": {
        "id": "7fb60124"
      },
      "source": [
        "#Step 4: Save and Visualize the Graph\n",
        "\n",
        "We'll save the graph for future use, as well as visualize it using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb98ff5",
      "metadata": {
        "id": "dfb98ff5"
      },
      "outputs": [],
      "source": [
        "# Set up the node labels\n",
        "labels = {url: data.get('label', '') for url, data in G.nodes(data=True)}\n",
        "\n",
        "# Save the graph so you don't have to re-run the code\n",
        "nx.write_gexf(G, \"example_crawl.gexf\")\n",
        "\n",
        "# Visualize the graph (optional: interactive version)\n",
        "plt.figure(figsize=(12, 12))\n",
        "pos = nx.spring_layout(G)  # Positions for all nodes\n",
        "\n",
        "# Draw nodes and edges\n",
        "nx.draw_networkx_nodes(G, pos, node_size=300, node_color='blue')\n",
        "nx.draw_networkx_edges(G, pos, arrowstyle=\"->\", arrowsize=10)\n",
        "\n",
        "# Draw node labels\n",
        "nx.draw_networkx_labels(G, pos, labels, font_size=10)\n",
        "\n",
        "# Show the graph\n",
        "plt.title(\"Web Crawl Graph for example.com\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab8c7ed",
      "metadata": {
        "id": "cab8c7ed"
      },
      "source": [
        "#Step 5: Interactivity\n",
        "\n",
        "Adding interactivity with libraries like pyvis to visualize the graph dynamically in the browser. If needed, install pyvis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b4f521",
      "metadata": {
        "id": "11b4f521"
      },
      "outputs": [],
      "source": [
        "pip install pyvis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45a8a9af",
      "metadata": {
        "id": "45a8a9af"
      },
      "source": [
        "This will create an interactive graph where you can explore the nodes and edges dynamically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dcb198f",
      "metadata": {
        "id": "4dcb198f"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "# Create a network visualization\n",
        "net = Network(notebook=True, height='750px', width='100%', directed=True)\n",
        "net.from_nx(G)  # Load the networkx graph\n",
        "net.show(\"example_crawl.html\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}